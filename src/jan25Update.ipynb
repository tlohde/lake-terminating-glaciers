{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I've done so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from glob import glob\n",
    "import dask.dataframe as da\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import rioxarray as rio\n",
    "import xarray as xr\n",
    "# from dem_utils import ArcticDEM\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "# import utils\n",
    "import cartopy.crs as ccrs\n",
    "import shapely\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.enums import Resampling\n",
    "import seaborn as sns\n",
    "# import odc.geo.xr\n",
    "from cycler import cycler\n",
    "prj = ccrs.Stereographic(\n",
    "    central_latitude=90,\n",
    "    central_longitude=-45,\n",
    "    true_scale_latitude=70\n",
    ")\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def hexbin(x, y, color, **kwargs):\n",
    "    # facetgrid hexbin func\n",
    "    cmap = sns.light_palette(color, as_cmap=True)\n",
    "    plt.hexbin(x, y, gridsize=30, cmap=cmap, **kwargs)\n",
    "    \n",
    "\n",
    "def demote_coords_to_vars(ds: xr.Dataset,\n",
    "                        coords: str,\n",
    "                        var_name: str):\n",
    "    '''\n",
    "    messy onliner to for reorganizing dataset.\n",
    "    e.g. dataset with two variables: a (dims: x, y, t) and b (dims: x, y)\n",
    "    this function will convert it to a dataset with \n",
    "    dimensions x, y and add as many `a` variables as there dim `t` is long\n",
    "    '''\n",
    "    return xr.merge([\n",
    "        ds.drop_vars([coords, var_name]),\n",
    "        xr.merge(\n",
    "            [ds[var_name].isel({coords:i}).rename(ds[coords][i].item())\n",
    "            for i in range(len(ds[coords]))], compat='override').drop_vars(coords)]\n",
    "                    )\n",
    "\n",
    "def add_geom_mask(ds, geom, buffer=200):\n",
    "    # buffer geometry, with square ends\n",
    "    buff_geom = geom.buffer(buffer, cap_style=3)\n",
    "    \n",
    "    # empty array of same x, y dim shape as merged\n",
    "    arr = np.zeros((ds.sizes['y'], ds.sizes['x']))\n",
    "    \n",
    "    # rasterize\n",
    "    burned = rasterize(shapes=[(buff_geom, 1)],\n",
    "                       fill=0,\n",
    "                       out=arr,\n",
    "                       transform=ds.rio.transform())\n",
    "    \n",
    "    # merged rasterized with all other dataarrays\n",
    "    merged = xr.merge([ds, xr.DataArray(data=burned,\n",
    "                                        dims=['y','x'],\n",
    "                                        coords={'y': ds.y,\n",
    "                                                'x': ds.x}).rename('buffer_aoi')])\n",
    "\n",
    "    return merged\n",
    "\n",
    "class site():\n",
    "    def __init__(self,\n",
    "                 id: int,\n",
    "                 vars: list=['sec', 'dem', 'sample', 'coreg_meta', 'stable_terrain', 'centreline']):\n",
    "        \n",
    "        '''\n",
    "        convenience class for opening output files from directory id\n",
    "        id = id number of study site directory\n",
    "        vars = list of variables to include\n",
    "        returns the opened files\n",
    "        '''\n",
    "        \n",
    "        directories = glob('../data/id*')\n",
    "        directory = [d for d in directories if f'id{id}' in d]\n",
    "        assert len(directory) == 1, 'too many or not enough'\n",
    "        self.directory = directory[0]\n",
    "\n",
    "        self.paths = {\n",
    "            'sec': os.path.join(self.directory, 'sec.zarr'),\n",
    "            'dem': os.path.join(self.directory, 'stacked_coregd.zarr'),\n",
    "            'sample': os.path.join(self.directory, 'sec_sample.parquet'),\n",
    "            'coreg_meta': os.path.join(self.directory, 'coregistration_metadata.parquet'),\n",
    "            'stable_terrain': os.path.join(self.directory, 'stable_terrain_mask.tif'),\n",
    "            'centreline': os.path.join(self.directory, glob('line*.geojson', root_dir=self.directory)[0])\n",
    "            }\n",
    "\n",
    "        to_remove = []\n",
    "        for k, v in self.paths.items():\n",
    "            if os.path.exists(v):\n",
    "                continue\n",
    "            else:\n",
    "                to_remove.append(k)\n",
    "        \n",
    "        [self.paths.pop(k) for k in to_remove]\n",
    "                \n",
    "        self.open_funcs = {\n",
    "            '.tif': rio.open_rasterio,\n",
    "            '.zarr': xr.open_zarr,\n",
    "            '.parquet': pd.read_parquet,\n",
    "            '.geojson': gpd.read_file\n",
    "        }\n",
    "        \n",
    "        for var in [var for var in vars if var in self.paths.keys()]:\n",
    "            _, extension = os.path.splitext(self.paths[var])\n",
    "            setattr(self, var, self.open_funcs[extension](self.paths[var]))\n",
    "        \n",
    "        try:\n",
    "            self.sec = demote_coords_to_vars(self.sec, 'result', 'sec')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "def get_line(d):\n",
    "    '''\n",
    "    grab centreline from directory\n",
    "    '''\n",
    "    with open(f'{d}/download_notes.txt', 'r') as notes:\n",
    "        notes = json.load(notes)\n",
    "        return wkt.loads(notes['centreline'])\n",
    "\n",
    "def get_ds(d):\n",
    "    '''\n",
    "    grab dem trend from directory\n",
    "    '''\n",
    "    _f = glob(f'{d}/coregistered/robust_spatial*.zarr')\n",
    "    assert len(_f) == 1, 'not enough / too many .zarr dems'\n",
    "    return xr.open_dataarray(_f[0], engine='zarr')\n",
    "\n",
    "def sample_along_line(d):\n",
    "    '''\n",
    "    sample dhdt xarray along centreline\n",
    "    '''\n",
    "    ds = get_ds(d)\n",
    "    line = get_line(d)\n",
    "    points = [line.interpolate(x/100, normalized=True) for x in range(0,100)]\n",
    "    distance = [line.project(p)/1000 for p in points]\n",
    "    x = [p.x for p in points] \n",
    "    y = [p.y for p in points]\n",
    "    df = pd.DataFrame({'distance (km)': distance,\n",
    "                       'x': x,\n",
    "                       'y': y})\n",
    "    idx = df.set_index('distance (km)').to_xarray()\n",
    "    return ds.interp(x=idx['x'],\n",
    "                     y=idx['y'])\n",
    "\n",
    "def plot_profile(d, ax):\n",
    "    '''\n",
    "    plot dhdt profile\n",
    "    '''\n",
    "    profile = sample_along_line(d)\n",
    "    profile.sel(result='slope').plot(c='k',\n",
    "                                     ax=ax)\n",
    "    high = profile.sel(result='slope').max()\n",
    "    low = profile.sel(result='slope').min()\n",
    "    span = high - low\n",
    "    ax.fill_between(profile['distance (km)'],\n",
    "                    y1 = profile.sel(result='low_slope'),\n",
    "                    y2 = profile.sel(result='high_slope'),\n",
    "                    color='lightgrey',\n",
    "                    alpha=0.8,\n",
    "                    label=\"95% CI\")\n",
    "    ax.set_ylim(low - (high-low),\n",
    "                high + (high-low))\n",
    "    ax.set_ylabel('dh/dt (m/yr)')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.axhline(0, c='k', lw=0.5, zorder=0)\n",
    "    ax.set_title(None)\n",
    "    \n",
    "def plot_xeqy(ax, c='gray'):\n",
    "    minx, maxx = ax.get_xlim()\n",
    "    miny, maxy = ax.get_ylim()\n",
    "    ax.plot([max(minx, miny), (min(maxx, maxy))],\n",
    "            [max(minx, miny), (min(maxx, maxy))], c=c, zorder=0)\n",
    "    \n",
    "\n",
    "def samples_groupby(samples, by=['where', 'region', 'lake_land', 'mid_bin']):\n",
    "    '''\n",
    "    group elevation samples by region/where/lake_land\n",
    "    useful for plotting scatter of lake SEC vs. land SEC for different groupings\n",
    "    e.g. symbolised by greenland/iceland (where) or by greenlandic region\n",
    "    '''    \n",
    "    lower = lambda x: np.quantile(x, 0.25)\n",
    "    upper = lambda x: np.quantile(x, 0.75)\n",
    "\n",
    "    ## add / remove region if needed\n",
    "    slope_by_z = samples.groupby(by,\n",
    "                                observed=True)['slope'].agg([lower,\n",
    "                                                            upper,\n",
    "                                                            'median']).reset_index()\n",
    "    by.remove('lake_land')\n",
    "    lake_land_df = pd.merge(\n",
    "        slope_by_z.loc[slope_by_z['lake_land']=='land'].rename(columns={'<lambda_0>':'land_lower',\n",
    "                                                                        '<lambda_1>':'land_upper',\n",
    "                                                                        'median': 'land_median'}),\n",
    "        slope_by_z.loc[slope_by_z['lake_land']=='lake'].rename(columns={'<lambda_0>':'lake_lower',\n",
    "                                                                        '<lambda_1>':'lake_upper',\n",
    "                                                                        'median': 'lake_median'}),\n",
    "        left_on=by,\n",
    "        right_on=by\n",
    "        )\n",
    "\n",
    "    ## nonsense for errorbars\n",
    "    lake_land_df['land_low_err'] = lake_land_df['land_median'] - lake_land_df['land_lower']\n",
    "    lake_land_df['land_up_err'] = lake_land_df['land_upper'] - lake_land_df['land_median']\n",
    "\n",
    "    lake_land_df['lake_low_err'] = lake_land_df['lake_median'] - lake_land_df['lake_lower']\n",
    "    lake_land_df['lake_up_err'] = lake_land_df['lake_upper'] - lake_land_df['lake_median']\n",
    "    \n",
    "    return lake_land_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic inputs metadata\n",
    "site_directories = glob('../data/id*')\n",
    "site_directories.remove('../data/id32_1327449x_-2520905y')  # removing dodgy 32\n",
    "\n",
    "centrelines = gpd.read_file('../data/streams_v3.geojson') # removing dodgy 32\n",
    "centrelines = centrelines.loc[centrelines.index != 32]\n",
    "\n",
    "basins = (gpd.read_file('../data/basins/Greenland_Basins_PS_v1.4.2.shp')\n",
    "          .dissolve('SUBREGION1'))\n",
    "centrelines = centrelines.sjoin(basins.drop(columns=['NAME', 'GL_TYPE']),\n",
    "                                how='left'\n",
    "                                ).rename(columns={'index_right': 'region'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface elevation change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** NOTE TO SELF: MANUALLY REMOVE `id32` AS THOSE RESULTS ARE CLEARLY JUNK **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meta data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab some meta data\n",
    "dem_ids = []\n",
    "dem_counts = []\n",
    "useful_dem_counts = []\n",
    "years = []\n",
    "for d in site_directories:\n",
    "    df= pd.read_parquet(\n",
    "        os.path.join(d, 'coregistration_metadata.parquet')\n",
    "        )\n",
    "    dem_ids += df['to_reg_dem_id'].unique().tolist()\n",
    "    dem_counts.append(len(df['to_reg_dem_id'].unique()))\n",
    "    useful_dem_counts.append(((df['nmad_after'] < 2) & (df['median_after'].abs() < 1)).sum())\n",
    "    years.append((df.index.min().year, df.index.max().year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the meta data\n",
    "print('number of study sites, by location and type')\n",
    "print(centrelines.groupby(['where', 'lake_land'])['geometry'].count().to_markdown())\n",
    "\n",
    "print(centrelines.groupby(['where', 'region', 'lake_land'])['geometry'].count().to_markdown())\n",
    "\n",
    "print(f'\\ntotal of {len(dem_ids)} _sections_ of DEMs used and coregistered (total of: {len(set(dem_ids))} tiles)')\n",
    "print(f'each of the 36 study sites on average used {np.mean(dem_counts):.0f}'\n",
    "      f'(mean) {np.median(dem_counts):.0f} (median) DEMs.',\n",
    "      '\\n(where used means coregistered)\\n')\n",
    "\n",
    "print('Coregistered DEMs were removed from subsequent analyses if they '\n",
    "      'failed to meet NMAD and MDOST of 2 m and ±1 m, respectively')\n",
    "print(f'the site with the fewest/most DEMs used {np.min(useful_dem_counts):.0f}/{np.max(useful_dem_counts):.0f}. '\n",
    "      f'mean/median: {np.mean(useful_dem_counts):.0f}/{np.median(useful_dem_counts):.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the coregistration metadata\n",
    "dfs = []\n",
    "for d in site_directories:\n",
    "    df = pd.read_parquet(\n",
    "        os.path.join(d, 'coregistration_metadata.parquet')\n",
    "        )\n",
    "    \n",
    "    ls_path = glob('line_*.geojson', root_dir=d)\n",
    "    assert len(ls_path)==1, 'oops'\n",
    "    ls = gpd.read_file(\n",
    "        os.path.join(d, ls_path[0])\n",
    "    )\n",
    "    \n",
    "    df['where'] = ls.loc[0, 'where']\n",
    "    df['lake_land'] = ls.loc[0, 'lake_land']\n",
    "    df['id'] = int(d.split('_')[0].split('id')[-1])\n",
    "    dfs.append(df)\n",
    "    \n",
    "coreg_meta = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot to coregistration metadata\n",
    "fig, axs = plt.subplots(ncols=2, figsize=[8,4])\n",
    "\n",
    "# MEDIAN\n",
    "sns.scatterplot(data=coreg_meta,\n",
    "                x='median_after',\n",
    "                y='median_before',\n",
    "                hue='lake_land',\n",
    "                style='where',\n",
    "                s=10,\n",
    "                markers=['o','s'],\n",
    "                ax=axs[0],\n",
    "                legend=False)\n",
    "\n",
    "axs[0].set(xlim=(-5, 5),\n",
    "       ylim=(-22, 25)\n",
    ")\n",
    "axs[0].axhline(0, c='gray', zorder=0, lw=0.5)\n",
    "axs[0].axvline(0, c='gray', zorder=0, lw=0.5)\n",
    "plot_xeqy(axs[0])\n",
    "\n",
    "axs[0].fill_between([-5, -1], -22, 25, color='lightgray', alpha=0.5)\n",
    "axs[0].fill_between([1, 5], -22, 25, color='lightgray', alpha=0.5)\n",
    "\n",
    "axs[0].set_title('Medians')\n",
    "\n",
    "## NMAD\n",
    "sns.scatterplot(data=coreg_meta,\n",
    "                x='nmad_after',\n",
    "                y='nmad_before',\n",
    "                hue='lake_land',\n",
    "                style='where',\n",
    "                s=10,\n",
    "                markers=['o','s'],\n",
    "                ax=axs[1],\n",
    "                legend=False)\n",
    "\n",
    "axs[1].set(xlim=(0, 5),\n",
    "           ylim=(0, 7.5)\n",
    ")\n",
    "plot_xeqy(axs[1])\n",
    "axs[1].fill_between([2, 7.5], 0, 7.5, color='lightgray', alpha=0.5)\n",
    "axs[1].set_title('NMAD')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffered each centreline by 200 m and extracted rates of SEC for all gridsquares in buffered region along with the _median_ elevation from the stack of coregistered DEMs.\n",
    "\n",
    "Plot of elevation against rate of SEC by Greenland/Iceland and lake/land."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in sample files, and assign basin SUBREGION\n",
    "\n",
    "sample_files = glob('../data/id*/sec_sample.parquet')\n",
    "sample_files.remove('../data/id32_1327449x_-2520905y/sec_sample.parquet')  # removing dodgy 32!\n",
    "\n",
    "# read in all sample files\n",
    "samples = pd.concat(\n",
    "    [pd.read_parquet(sf) for sf in sample_files]\n",
    ")\n",
    "# add greenland basin region to samples\n",
    "samples['id'] = samples['id'].astype(int)\n",
    "samples = samples.merge(centrelines['region'], left_on='id', right_index=True)\n",
    "\n",
    "print(f'there are {len(samples)} sample points in total\\nand by where/type the breakdown is:')\n",
    "print(samples.groupby(['where','lake_land'])['z_median'].count().to_markdown())\n",
    "print(samples.groupby(['where', 'region', 'lake_land'])['z_median'].count().to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEC against elevation by lake/land and greenland basin\n",
    "fg = sns.FacetGrid(samples.loc[samples['where']=='greenland'],\n",
    "                    col='region',\n",
    "                    row='lake_land',\n",
    "                    hue='lake_land',\n",
    "                    hue_order=['lake', 'land'],\n",
    "                    row_order=['lake', 'land'],\n",
    "                    col_order=centrelines['region'].value_counts().index.tolist()\n",
    ")\n",
    "fg.map(hexbin, \"slope\", \"z_median\", extent=[-10,5,0,1600])\n",
    "\n",
    "for ax in fg.axes.flat:\n",
    "    t = ax.get_title()\n",
    "    t = t.replace('region = ', \"\").replace('lake_land = ', \"\")\n",
    "    ax.set_title(t)\n",
    "    ax.axvline(0, c='k', lw=0.5)\n",
    "\n",
    "fg.set(xlabel='Surface elevation change (m/yr)',\n",
    "       ylabel='Elevation (m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEC against elevation by lake/land and greenland/iceland\n",
    "\n",
    "fg = sns.FacetGrid(samples,\n",
    "                    row='where',\n",
    "                    col='lake_land',\n",
    "                    hue='lake_land',\n",
    "                    hue_order=['lake', 'land'],\n",
    "                    row_order=['greenland', 'iceland'],\n",
    "                    col_order=['lake', 'land'],\n",
    ")\n",
    "fg.map(hexbin, \"slope\", \"z_median\", extent=[-10,5,0,1600])\n",
    "\n",
    "for ax in fg.axes.flat:\n",
    "    t = ax.get_title()\n",
    "    t = t.replace('where = ', \"\").replace('lake_land = ', \"\")\n",
    "    ax.set_title(t.title())\n",
    "    ax.axvline(0, c='k', lw=0.5)\n",
    "\n",
    "fg.set(xlabel='Surface elevation change (m/yr)',\n",
    "       ylabel='Elevation (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- bin elevations into 20 m (or 50 or whatever) wide buckets\n",
    "- group all the samples by:\n",
    "    - mid value of this bucket\n",
    "    - iceland / greenland\n",
    "    - greenland region\n",
    "    - lake / land\n",
    "- for each group compute the median, lower quartile and upper quartile of the rates of SEC\n",
    "\n",
    "for compatability with `plt.errorbar()` need to add/subtract the upper/lower quartiles from the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin elevations\n",
    "bin_width = 25\n",
    "samples['z_bins'] = pd.cut(samples['z_median'], np.arange(12.5, 1812.5, bin_width))\n",
    "samples['mid_bin'] = samples['z_bins'].apply(lambda x: int(x.mid)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter of lake vs land SEC for GREENLAND ONLY, coloured by elevation, symbolised by region\n",
    "lake_land_df = samples_groupby(samples, ['where', 'region', 'lake_land', 'mid_bin'])\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "sns.scatterplot(data=lake_land_df.loc[lake_land_df['where']=='greenland'],\n",
    "                x='lake_median',\n",
    "                y='land_median',\n",
    "                style='region',\n",
    "              #   markers=['o','s'],\n",
    "                hue='mid_bin',\n",
    "                palette='Blues',\n",
    "                ec='k',\n",
    "                ax=ax,\n",
    "                legend=\"full\",\n",
    "                )\n",
    "\n",
    "# quick sort of legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[-4:], labels[-4:])\n",
    "\n",
    "ax.errorbar(x='lake_median',\n",
    "            y='land_median',\n",
    "            yerr=np.c_[lake_land_df['land_low_err'].values,\n",
    "                       lake_land_df['land_up_err'].values].T,\n",
    "            xerr=np.c_[lake_land_df['lake_low_err'].values,\n",
    "                       lake_land_df['lake_up_err'].values].T,\n",
    "            data=lake_land_df,\n",
    "            fmt='none',\n",
    "            ecolor='lightgray',\n",
    "            elinewidth=0.5,\n",
    "            zorder=0)\n",
    "\n",
    "# limits and aspect ratio and x=y line\n",
    "ax.set(\n",
    "  xlim=(-7.5, 0.5),\n",
    "  ylim=(-7.5, 0.5),\n",
    "  xlabel='Lakes : Surface elevation change (m/yr)',\n",
    "  ylabel='Land : Surface elevation change (m/yr)')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "ax.annotate(f'elevation bin width: {bin_width} m',\n",
    "            xy=(0.99, 0.01),\n",
    "            ha='right',\n",
    "            xycoords='axes fraction')\n",
    "\n",
    "plot_xeqy(ax)\n",
    "\n",
    "\n",
    "########## elevation colorbar\n",
    "norm = plt.Normalize(*lake_land_df['mid_bin'].agg(['min', 'max']))\n",
    "cmap = plt.get_cmap('Blues')\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "\n",
    "pos = ax.get_position()\n",
    "cax = fig.add_axes([pos.x1 + 0.01,\n",
    "                    pos.y0,\n",
    "                    0.03,\n",
    "                    pos.height])\n",
    "\n",
    "cb = plt.colorbar(sm, cax=cax)\n",
    "cax.set_ylabel('Elevation (m)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_land_df['diff_lake_sub_land'] = lake_land_df['lake_median'] - lake_land_df['land_median']\n",
    "\n",
    "ax = sns.histplot(data=lake_land_df,\n",
    "                 x='diff_lake_sub_land',\n",
    "                 hue='region',\n",
    "                #  palette=['tab:green', 'tab:pink'],\n",
    "                 binwidth=0.2,\n",
    "                 multiple='stack'\n",
    "                 )\n",
    "\n",
    "ax.axvline(0, c='k', zorder=0, lw=2, ls=':')\n",
    "\n",
    "ax.set_xlabel('<--- more thinning at lake <---| lake SEC - land SEC |---> more thinning at land --->')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "fg = sns.displot(samples.loc[samples['where']=='greenland'].sample(1000),\n",
    "                 x='slope',\n",
    "                 hue='region',\n",
    "                 common_norm=False,\n",
    "                 kind='kde',\n",
    "                 col='lake_land',\n",
    "                 col_order=['lake', 'land'])\n",
    "\n",
    "fg.set(ylim=(0, 1.5),\n",
    "       xlim=(-5, 5),\n",
    "       xlabel=('SEC (m/yr)'))\n",
    "\n",
    "for ax in fg.axes.flat:\n",
    "    ax.axvline(0, c='gray', lw=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter of lake vs land SEC for coloured by elevation, symbolised by greenland/iceland\n",
    "\n",
    "lake_land_df = samples_groupby(samples, ['where', 'lake_land', 'mid_bin'])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "sns.scatterplot(data=lake_land_df,\n",
    "                x='lake_median',\n",
    "                y='land_median',\n",
    "                style='where',\n",
    "                markers=['o','s'],\n",
    "                hue='mid_bin',\n",
    "                palette='Blues',\n",
    "                ec='k',\n",
    "                ax=ax,\n",
    "                legend='brief'\n",
    "                )\n",
    "\n",
    "# quick sort of legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[-2:], labels[-2:])\n",
    "\n",
    "ax.errorbar(x='lake_median',\n",
    "            y='land_median',\n",
    "            yerr=np.c_[lake_land_df['land_low_err'].values,\n",
    "                       lake_land_df['land_up_err'].values].T,\n",
    "            xerr=np.c_[lake_land_df['lake_low_err'].values,\n",
    "                       lake_land_df['lake_up_err'].values].T,\n",
    "            data=lake_land_df,\n",
    "            fmt='none',\n",
    "            ecolor='lightgray',\n",
    "            elinewidth=0.5,\n",
    "            zorder=0)\n",
    "\n",
    "# limits and aspect ratio and x=y line\n",
    "ax.set(xlim=(-6, 0.5),\n",
    "       ylim=(-6, 0.5),\n",
    "       xlabel='Lakes : Surface elevation change (m/yr)',\n",
    "       ylabel='Land : Surface elevation change (m/yr)')\n",
    "ax.set_aspect('equal')\n",
    "plot_xeqy(ax)\n",
    "\n",
    "ax.annotate(f'elevation bin width: {bin_width} m',\n",
    "            xy=(0.99, 0.01),\n",
    "            ha='right',\n",
    "            xycoords='axes fraction')\n",
    "\n",
    "########## elevation colorbar\n",
    "norm = plt.Normalize(*lake_land_df['mid_bin'].agg(['min', 'max']))\n",
    "cmap = plt.get_cmap('Blues')\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "\n",
    "pos = ax.get_position()\n",
    "cax = fig.add_axes([pos.x1 + 0.01,\n",
    "                    pos.y0,\n",
    "                    0.03,\n",
    "                    pos.height])\n",
    "\n",
    "cb = plt.colorbar(sm, cax=cax)\n",
    "cax.set_ylabel('Elevation (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_cl_files = glob('../results/velocity/centreline_trend/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_from_path(path):\n",
    "    return int(os.path.basename(path).split('_')[0].split('id')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "24 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "33 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "1 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "23 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "27 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "30 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "31 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "6 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "7 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "11 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "2 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "13 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "3 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "28 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "16 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "8 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "17 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "12 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "10 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "22 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "14 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "18 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "4 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "25 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "29 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "0 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "5 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "34 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "20 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n",
      "35 Index(['slope', 'intercept', 'low_slope', 'high_slope', 'id', 'where',\n",
      "       'lake_land'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "cl_trend_dfs = []\n",
    "for f in v_cl_files:\n",
    "    id = id_from_path(f)\n",
    "    if id in [19, 32]: continue\n",
    "    _df = pd.read_parquet(f)\n",
    "    _df['id'] = id\n",
    "    _df['where'] = centrelines.loc[id,'where']\n",
    "    _df['lake_land'] = centrelines.loc[id, 'lake_land']\n",
    "    cl_trend_dfs.append(_df)\n",
    "    print(id, _df.columns)\n",
    "    \n",
    "cl_trend = pd.concat(cl_trend_dfs).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "zarr_dict = dict(\n",
    "    zip(\n",
    "        [int(os.path.basename(z).split('_')[0].split('id')[-1]) for z in v_zarrs],\n",
    "        v_zarrs\n",
    "    ))\n",
    "\n",
    "\n",
    "grps = centrelines.groupby(['where', 'lake_land', 'region'])\n",
    "group = list(grps.groups.keys())\n",
    "gidx = list(grps.groups.values())\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "ldfs = []\n",
    "for k, v in grps.groups.items():\n",
    "    zarr_files = [zarr_dict[z] for z in v if z in zarr_dict.keys()]\n",
    "    # print(k, v, zarr_files)\n",
    "    \n",
    "    for id, zarr in zip(v, zarr_files):\n",
    "        # print(id, zarr)\n",
    "        2+2\n",
    "        with xr.open_zarr(zarr)['v_trend'] as ds:\n",
    "            ldf = (sample_along_line(ds)\n",
    "                   .drop_vars(['mapping','x','y'])\n",
    "                   .to_dataframe()\n",
    "                   .unstack('result')['v_trend']\n",
    "                   .reset_index())\n",
    "            ldf['where'] = k[0]\n",
    "            ldf['lake_land'] = k[1]\n",
    "            ldf['region'] = k[2]\n",
    "            ldf['id'] = id\n",
    "            ldf['distance (km)'] = ldf['distance (km)'].round(2)\n",
    "            ldfs.append(ldf)\n",
    "\n",
    "line_df = pd.concat(ldfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spread of velocity trends in greenland\n",
    "fg = sns.relplot(line_df.loc[line_df['where']=='greenland'],\n",
    "                 x='distance (km)',\n",
    "                 y='slope',\n",
    "                 hue='region',\n",
    "                 col='lake_land',\n",
    "                 estimator='mean',\n",
    "                 errorbar='sd',\n",
    "                 kind='line')\n",
    "\n",
    "fg.map(plt.axhline, y=0, c='gray', lw=0.5)\n",
    "fg.set(xlim=(-0.25, 10.25),\n",
    "       xlabel='Distance from terminus (km)',\n",
    "       ylabel='Velocity trend (m/yr^-2)')\n",
    "\n",
    "for ax in fg.axes.flat:\n",
    "    t = ax.get_title()\n",
    "    ax.set_title(t.replace('lake_land = ', '').title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_df_split = samples_groupby(line_df, by=['where', 'region', 'lake_land', 'distance (km)'])\n",
    "# scatter of lake vs land SEC for GREENLAND ONLY, coloured by elevation, symbolised by region\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(figsize=[8,8])\n",
    "sns.scatterplot(data=line_df_split.loc[line_df_split['where']=='greenland'],\n",
    "                x='lake_median',\n",
    "                y='land_median',\n",
    "                style='region',\n",
    "              #   markers=['o','s'],\n",
    "                hue='distance (km)',\n",
    "                palette='Blues',\n",
    "                ec='k',\n",
    "                ax=ax,\n",
    "                legend=\"full\",\n",
    "                )\n",
    "\n",
    "# quick sort of legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[-2:], labels[-2:])\n",
    "\n",
    "ax.errorbar(x='lake_median',\n",
    "            y='land_median',\n",
    "            yerr=np.c_[line_df_split['land_low_err'].values,\n",
    "                       line_df_split['land_up_err'].values].T,\n",
    "            xerr=np.c_[line_df_split['lake_low_err'].values,\n",
    "                       line_df_split['lake_up_err'].values].T,\n",
    "            data=line_df_split,\n",
    "            fmt='none',\n",
    "            ecolor='lightgray',\n",
    "            elinewidth=0.5,\n",
    "            zorder=0)\n",
    "\n",
    "# limits and aspect ratio and x=y line\n",
    "ax.set(\n",
    "  xlim=(-4, 1.5),\n",
    "  ylim=(-4, 1.5),\n",
    "  xlabel='Lakes : Velocity trend (m yr^-2)',\n",
    "  ylabel='Land : Velocity trend (m yr^-2)')\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# ax.annotate(f'elevation bin width: {bin_width} m',\n",
    "#             xy=(0.99, 0.01),\n",
    "#             ha='right',\n",
    "#             xycoords='axes fraction')\n",
    "\n",
    "plot_xeqy(ax)\n",
    "\n",
    "\n",
    "########## elevation colorbar\n",
    "norm = plt.Normalize(*line_df_split['distance (km)'].agg(['min', 'max']))\n",
    "cmap = plt.get_cmap('Blues')\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "\n",
    "pos = ax.get_position()\n",
    "cax = fig.add_axes([pos.x1 + 0.01,\n",
    "                    pos.y0,\n",
    "                    0.03,\n",
    "                    pos.height])\n",
    "\n",
    "cb = plt.colorbar(sm, cax=cax)\n",
    "cax.set_ylabel('Distance from terminus (km)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fixing centreliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from velocity_utils import CentreLiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://127.0.0.1:8787/status\n",
      "getting cubes from itslive\n",
      "sampling along centreline\n",
      "filtering velocity cube\n",
      "computing trend along centreline\n",
      "written to ../results/velocity/centreline_trend/id26_trend.parquet\n",
      "generating annual median field\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s1759665/micromamba/envs/paper2/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1563: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n",
      "/home/s1759665/micromamba/envs/paper2/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1563: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n",
      "/home/s1759665/micromamba/envs/paper2/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1563: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n",
      "/home/s1759665/micromamba/envs/paper2/lib/python3.12/site-packages/numpy/lib/nanfunctions.py:1563: RuntimeWarning: All-NaN slice encountered\n",
      "  return function_base._ureduce(a,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "written to ../results/velocity/annual_fields/id26_vField.zarr\n"
     ]
    }
   ],
   "source": [
    "with LocalCluster() as cluster:\n",
    "    client = cluster.get_client()\n",
    "    print(client.dashboard_link)\n",
    "    \n",
    "    params = {'ddt_range': [pd.Timedelta(td) for td in ('335d', '395d')],\n",
    "            'middate_range': (pd.to_datetime('2010-01-01'), pd.Timestamp.now())}\n",
    "\n",
    "\n",
    "    V = CentreLiner(geo=centrelines.loc[26, 'geometry'],\n",
    "                    buff_dist=3000,\n",
    "                    index=26,\n",
    "                    sample_centreline=True,\n",
    "                    filter=True,\n",
    "                    get_robust_trend='line',\n",
    "                    get_annual_quantiles=True,\n",
    "                    get_rgb=False,\n",
    "                    **params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paper2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
